{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MushroomClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RqE6mclYuJi",
        "colab_type": "text"
      },
      "source": [
        "# Mushroom classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLM-_B8-ZlZ_",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVu2expsYzoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q pydot\n",
        "!pip install graphviz \n",
        "!apt-get install graphviz\n",
        "\n",
        "import os\n",
        "import json\n",
        "import keras\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from random import shuffle\n",
        "from keras import optimizers\n",
        "from keras import callbacks\n",
        "from keras.utils import plot_model\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "HEIGHT = 224\n",
        "WIDTH = 224\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j72UCTNZn-E",
        "colab_type": "text"
      },
      "source": [
        "# Model\n",
        "Import ResNet (image classification) with weights from ImageNet but without top layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO9IivF2Zv1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(freeze_layers_from='base_model'):\n",
        "  base_model = ResNet50(weights='imagenet', include_top=False,input_shape=(224,224,3))\n",
        "  # Add final layers\n",
        "  x = base_model.output\n",
        "  x = Flatten()(x)\n",
        "  predictions = Dense(16, activation='softmax', name='fc1000')(x)\n",
        "\n",
        "  model = Model(input=base_model.input, output=predictions)\n",
        "  print('Number of layers: {}'.format(len(model.layers)))\n",
        "  # Freeze some layers\n",
        "  if freeze_layers_from is not None:\n",
        "      if freeze_layers_from == 'base_model':\n",
        "          print ('   Freezing base model layers')\n",
        "          for layer in base_model.layers:\n",
        "              layer.trainable = False\n",
        "      else:\n",
        "          for i, layer in enumerate(model.layers):\n",
        "              print(i, layer.name)\n",
        "          print ('   Freezing from layer 0 to ' + str(freeze_layers_from))\n",
        "          for layer in model.layers[:freeze_layers_from]:\n",
        "             layer.trainable = False\n",
        "          for layer in model.layers[freeze_layers_from:]:\n",
        "             layer.trainable = True\n",
        "  \n",
        "  opt = optimizers.Adam(lr=0.0001)\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTrULQAeaIXz",
        "colab_type": "text"
      },
      "source": [
        "# Load dataset\n",
        "\n",
        "Training split:\n",
        "- 2076 images  \n",
        "- 39 classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8-gMjNp3IvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize(input_image=224):\n",
        "  input_image = Image.open(input_image).resize((HEIGHT, WIDTH),Image.LANCZOS)\n",
        "  return input_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G_8TCl-hL-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip  /gdrive/'My Drive'/dataset_small\n",
        "!ls /gdrive/'My Drive'\n",
        "TRAIN_SIZE= 7415\n",
        "VALIDATION_SIZE= 1301"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbdagCZTaO4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data generator\n",
        "datagen = ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input,\n",
        "        zoom_range=0.2,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "\t\t    height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        validation_split=0.15,\n",
        "    \t  horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "\t\t    fill_mode=\"nearest\"\n",
        "        )\n",
        "\n",
        "\n",
        "# this is a generator that will read pictures found in\n",
        "# subfolders of 'data/train', and indefinitely generate\n",
        "# batches of augmented image data\n",
        "train_generator = datagen.flow_from_directory(\n",
        "        'dataset', # this is the target directory\n",
        "        shuffle=True,\n",
        "        seed=82,\n",
        "        interpolation='bicubic',\n",
        "        subset='training',\n",
        "        target_size=(224,224),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
        "\n",
        "# this is a similar generator, for validation data\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "        'dataset',\n",
        "        batch_size=BATCH_SIZE,\n",
        "        target_size=(224,224),\n",
        "        shuffle=False,\n",
        "        subset='validation',\n",
        "        interpolation='bicubic',\n",
        "        class_mode='categorical')\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSwiV3NukNu-",
        "colab_type": "text"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK02xhLikT_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce_lr= callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10)\n",
        "early_stopping= callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "checkpoint= callbacks.ModelCheckpoint('best_model.hdf5',monitor='val_loss',save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsWbwJzHkSZD",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF6Co6n_kQzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model()\n",
        "\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=TRAIN_SIZE // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[reduce_lr, early_stopping, checkpoint],\n",
        "        verbose=1,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=VALIDATION_SIZE //BATCH_SIZE)\n",
        "\n",
        "model.save('classifier.h5')\n",
        "\n",
        "# clear_output()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF5viJkDbPeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_map = (train_generator.class_indices)\n",
        "print(label_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERT-wWqd8RVg",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avirenvu8TGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(model, img):\n",
        "  image = resize(img)\n",
        "  x = np.expand_dims(image, axis=0) \n",
        "  x = preprocess_input(x)\n",
        "  print('Result: {}'.format(np.argmax(model.predict(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hag08kXboXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_model(model, './dataset/amanita_muscaria/amanita_muscaria_150.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}